"""
This is a pipeline 'data_unit_tests'
generated using Kedro 0.19.14
"""

import os

import logging

import pandas as pd

import great_expectations as gx

from ydata_profiling import ProfileReport
from ydata_profiling.expectations_report import ExpectationsReport

import mlflow

from data_ingestion.auto_expectations import ExpectationsReportV3



logger = logging.getLogger(__name__)


def get_validation_results(checkpoint_result):
    # validation_result is a dictionary containing one key-value pair
    validation_result_key, validation_result_data = next(iter(checkpoint_result["run_results"].items()))

    # Accessing the 'actions_results' from the validation_result_data
    validation_result_ = validation_result_data.get('validation_result', {})

    # Accessing the 'results' from the validation_result_data
    results = validation_result_["results"]
    meta = validation_result_["meta"]
    use_case = meta.get('expectation_suite_name')
    
    
    df_validation = pd.DataFrame(
        {}
        ,columns=[
            "Success","Expectation Type","Column","Column Pair","Max Value","Min Value","Element Count"
            ,"Unexpected Count","Unexpected Percent","Value Set","Unexpected Value","Observed Value"
        ]
    )
    
    
    for result in results:
        # Process each result dictionary as needed
        success = result.get('success', '')
        expectation_type = result.get('expectation_config', {}).get('expectation_type', '')
        column = result.get('expectation_config', {}).get('kwargs', {}).get('column', '')
        column_A = result.get('expectation_config', {}).get('kwargs', {}).get('column_A', '')
        column_B = result.get('expectation_config', {}).get('kwargs', {}).get('column_B', '')
        value_set = result.get('expectation_config', {}).get('kwargs', {}).get('value_set', '')
        max_value = result.get('expectation_config', {}).get('kwargs', {}).get('max_value', '')
        min_value = result.get('expectation_config', {}).get('kwargs', {}).get('min_value', '')

        element_count = result.get('result', {}).get('element_count', '')
        unexpected_count = result.get('result', {}).get('unexpected_count', '')
        unexpected_percent = result.get('result', {}).get('unexpected_percent', '')
        observed_value = result.get('result', {}).get('observed_value', '')
        if type(observed_value) is list:
            #sometimes observed_vaue is not iterable
            unexpected_value = [item for item in observed_value if item not in value_set]
        else:
            unexpected_value=[]
        
        df_validation = pd.concat([
            df_validation
            ,pd.DataFrame.from_dict([{
                "Success" :success
                ,"Expectation Type" :expectation_type
                ,"Column" : column
                ,"Column Pair" : (column_A, column_B)
                ,"Max Value" :max_value
                ,"Min Value" :min_value
                ,"Element Coun# Update feature store version
            versions["target_feature"] += 1t" :element_count
                ,"Unexpected Count" :unexpected_count
                ,"Unexpected Percent":unexpected_percent
                ,"Value Set" : value_set
                ,"Unexpected Value" :unexpected_value
                ,"Observed Value" :observed_value
            }])
        ], ignore_index=True)
        
    return df_validation


def test_data(df):
    context_root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../../gx"))
    context = gx.get_context(context_root_dir=context_root_dir)
    datasource_name = "stock_datasource"
    try:
        datasource = context.sources.add_pandas(datasource_name)
        logger.info("Data Source created.")
    except:
        logger.info("Data Source already exists.")
        datasource = context.datasources[datasource_name]

    data_asset_name = "stock_asset"
    try:
        data_asset = datasource.add_dataframe_asset(name=data_asset_name, dataframe=df)
    except:
        print("The data asset alread exists. The required one will be loaded.")
        data_asset = datasource.get_asset(data_asset_name)

    # Generate auto expectations from ydata_profiling profile
    profile = ProfileReport(df, title="Auto Profiling Report", minimal=True)
    ExpectationsReport.to_expectation_suite = ExpectationsReportV3.to_expectation_suite

    auto_suite = profile.to_expectation_suite(
        datasource_name=datasource_name,
        data_asset_name=data_asset_name,
        suite_name="stock_auto_suite",
        data_context=context,
        run_validation=False,
        dataframe=df
    )

    context.add_or_update_expectation_suite(expectation_suite=auto_suite)

    # Change autogenerated expectations
    exp_columns = {'close','high','low','open','volume'}
    for exp in auto_suite.expectations:
        if (
            exp.expectation_type == "expect_column_values_to_be_between"
            and exp.kwargs.get("column") in exp_columns
        ):
            # Modify values
            exp.kwargs['min_value'] = 0

    context.save_expectation_suite(
        expectation_suite=auto_suite,
        expectation_suite_name="stock_auto_suite"
    )

    batch_request = data_asset.build_batch_request(dataframe=df)
    validator = context.get_validator(
        batch_request=batch_request,
        expectation_suite=auto_suite,
    )

    # Run validation with combined expectations
    checkpoint = gx.checkpoint.SimpleCheckpoint(
        name="combined_checkpoint",
        data_context=context,
        validations=[{
            "batch_request": batch_request,
            "expectation_suite_name": validator.expectation_suite.expectation_suite_name,
        }]
    )
    checkpoint_result = checkpoint.run()

    # Process validation results and log to MLflow
    df_validation = get_validation_results(checkpoint_result)

    with mlflow.start_run(run_name="data_validation", nested=True):
        mlflow.log_param("expectation_suite", validator.expectation_suite.expectation_suite_name)
        mlflow.log_metric("success_count", df_validation["Success"].sum())
        mlflow.log_metric("failure_count", len(df_validation) - df_validation["Success"].sum())

    # Return results or raise error based on validation
    if df_validation["Success"].all():
        logger.info("All data tests passed")
    else:
        logger.warning("Some data tests failed")
        # raise AssertionError("Data validation failed")

    return df_validation
